{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing multiclass LightGBM classifiers for imbalanced darasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score,accuracy_score,confusion_matrix,roc_auc_score\n",
    "from OneVsRestClassifierCustomizedLoss import *\n",
    "from FocalLoss import FocalLoss\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_classes=3,\n",
    "                           n_samples=2000, \n",
    "                           n_features=2,\n",
    "                           n_informative=2,\n",
    "                           n_redundant =0,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[.01, .01, .98], \n",
    "                           flip_y=.01, \n",
    "                           random_state=42)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_label, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class 0', 'Class 1', 'Class 2']\n"
     ]
    }
   ],
   "source": [
    "classes =[]\n",
    "labeles=np.unique(y_label)\n",
    "for v in labeles:\n",
    "    classes.append('Class '+ str(v))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass lightgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "Number of class for initial score error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b6f67d31c359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    848\u001b[0m                                         \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                                         \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[0;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2056\u001b[0m             \u001b[0mparams_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2058\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0m\u001b[0;32m   2059\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Number of class for initial score error"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "init_score=np.full_like(y_train, 0.0, dtype=float)\n",
    "clf.fit(X_train, y_train,init_score=init_score)\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "pred_accuracy_score = accuracy_score(y_test, y_test_pred)\n",
    "pred_recall_score = recall_score(y_test, y_test_pred, average='macro')\n",
    "print('Prediction accuracy', pred_accuracy_score,' recall ', pred_recall_score)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred, labels=labeles)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass lightgbm with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "loss_fun = lambda y_true, y_pred: (loss.grad(y_true, special.expit(y_pred)), loss.hess(y_true, special.expit(y_pred)))\n",
    "\n",
    "# Not using early stopping\n",
    "estimator = lgb.LGBMClassifier(objective=loss_fun)\n",
    "clf = OneVsRestClassifierCustomizedLoss(estimator=estimator, loss=loss)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# For using early stopping, uncomment the following three lines and comment the two above\n",
    "#estimator = lgb.LGBMClassifier(objective=loss_fun,metric='custom')\n",
    "#clf = OneVsRestClassifierCustomizedLoss(estimator=estimator, loss=loss)\n",
    "#eval_metric = lambda y_true, y_pred: ('focal_loss', loss(y_true, special.expit(y_pred)).sum(), False)\n",
    "#fit_params = {'eval_set': [(X_test, y_test)], 'eval_metric': eval_metric}\n",
    "#clf.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "pred_accuracy_score = accuracy_score(y_test, y_test_pred)\n",
    "pred_recall_score = recall_score(y_test, y_test_pred, average='macro')\n",
    "print('prediction accuracy', pred_accuracy_score,' recall ', pred_recall_score)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred, labels=labeles)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_classes=2,\n",
    "                           n_samples=200, \n",
    "                           n_features=2,\n",
    "                           n_informative=2,\n",
    "                           n_redundant =0,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[.5, .5], \n",
    "                           flip_y=.0, \n",
    "                           random_state=42)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_label, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "yhat = clf.predict(X_train)\n",
    "y_score = clf.predict(X_train, raw_score=True)\n",
    "print(y_score)\n",
    "\n",
    "\n",
    "clf2 = clf\n",
    "clf2.fit(X_train, y_train, init_score=y_score)\n",
    "y_score2 = clf2.predict(X_train,raw_score=True) + y_score\n",
    "\n",
    "print(y_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_classes=2,\n",
    "                           n_samples=2000, \n",
    "                           n_features=2,\n",
    "                           n_informative=2,\n",
    "                           n_redundant =0,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[.4, .6], \n",
    "                           flip_y=.01, \n",
    "                           random_state=42)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(y)\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(X, y_label, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 1400, number of used features: 2\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's logloss: 0.272019\tval's logloss: 0.302937\n",
      "[200]\tfit's logloss: 0.150033\tval's logloss: 0.209395\n",
      "[300]\tfit's logloss: 0.0982027\tval's logloss: 0.183269\n",
      "Early stopping, best iteration is:\n",
      "[358]\tfit's logloss: 0.0807425\tval's logloss: 0.181665\n",
      "\n",
      "Test's ROC AUC: 0.96233\n",
      "Test's logloss: 0.18167\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def logloss_objective(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    p = special.expit(preds)\n",
    "    grad = p - y\n",
    "    hess = p * (1 - p)\n",
    "    return grad, hess\n",
    "\n",
    "def logloss_metric(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    p = special.expit(preds)\n",
    "\n",
    "    ll = np.empty_like(p)\n",
    "    pos = y == 1\n",
    "    ll[pos] = np.log(p[pos])\n",
    "    ll[~pos] = np.log(1 - p[~pos])\n",
    "\n",
    "    is_higher_better = False\n",
    "    return 'logloss', -ll.mean(), is_higher_better\n",
    "\n",
    "def logloss_init_score(y):\n",
    "    p = y.mean()\n",
    "    p = np.clip(p, 1e-15, 1 - 1e-15)  # never hurts\n",
    "    log_odds = np.log(p / (1 - p))\n",
    "    return log_odds\n",
    "\n",
    "fit = lgb.Dataset(\n",
    "    X_fit, y_fit,\n",
    "    init_score=np.full_like(y_fit, logloss_init_score(y_fit), dtype=float)\n",
    ")\n",
    "\n",
    "val = lgb.Dataset(\n",
    "    X_val, y_val,\n",
    "    init_score=np.full_like(y_val, logloss_init_score(y_fit), dtype=float),\n",
    "    reference=fit\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params={'learning_rate': 0.01},\n",
    "    train_set=fit,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=100,\n",
    "    fobj=logloss_objective,\n",
    "    feval=logloss_metric\n",
    ")\n",
    "\n",
    "# Notice the change here\n",
    "y_pred = special.expit(logloss_init_score(y_fit) + model.predict(X_test))\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {metrics.roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {metrics.log_loss(y_test, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_iterations is set=10000, num_boost_round=10000 will be ignored. Current value: num_iterations=10000\n",
      "\n",
      "Test's ROC AUC: 0.04481\n",
      "Test's logloss: 0.69805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "d:\\apps\\python_3_8_6\\lib\\site-packages\\lightgbm\\sklearn.py:925: UserWarning: Cannot compute class probabilities or labels due to the usage of customized objective function.\n",
      "Returning raw scores instead.\n",
      "  warnings.warn(\"Cannot compute class probabilities or labels \"\n"
     ]
    }
   ],
   "source": [
    "def logloss_objective_sk(preds, y):\n",
    "    p = special.expit(preds)\n",
    "    grad = p - y\n",
    "    hess = p * (1 - p)\n",
    "    return grad, hess\n",
    "\n",
    "def logloss_metric_sk(preds, y):\n",
    "    p = special.expit(preds)\n",
    "\n",
    "    ll = np.empty_like(p)\n",
    "    pos = y == 1\n",
    "    ll[pos] = np.log(p[pos])\n",
    "    ll[~pos] = np.log(1 - p[~pos])\n",
    "\n",
    "    is_higher_better = False\n",
    "    return 'logloss', -ll.mean(), is_higher_better\n",
    "\n",
    "\n",
    "model = lgb.LGBMClassifier(objective=logloss_objective_sk,num_boost_round=10000)\n",
    "\n",
    "init_score=np.full_like(y_fit, logloss_init_score(y_fit), dtype=float)\n",
    "eval_init_score= [np.full_like(y_val, logloss_init_score(y_fit), dtype=float)]\n",
    "\n",
    "model.fit(X_fit, y_fit,\n",
    "              init_score = init_score,\n",
    "              eval_init_score = eval_init_score,\n",
    "              early_stopping_rounds=10,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric=logloss_metric_sk,\n",
    "              verbose=False)\n",
    "\n",
    "# Notice the change here\n",
    "#y_pred = special.expit(model.predict(X_test))\n",
    "y_pred = special.expit(logloss_init_score(y_fit) + model.predict(X_test))\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {metrics.roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {metrics.log_loss(y_test, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 826, number of negative: 574\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 1400, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.590000 -> initscore=0.363965\n",
      "[LightGBM] [Info] Start training from score 0.363965\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's binary_logloss: 0.272019\tval's binary_logloss: 0.302937\n",
      "[200]\tfit's binary_logloss: 0.150033\tval's binary_logloss: 0.209395\n",
      "[300]\tfit's binary_logloss: 0.0982027\tval's binary_logloss: 0.183269\n",
      "Early stopping, best iteration is:\n",
      "[358]\tfit's binary_logloss: 0.0807425\tval's binary_logloss: 0.181665\n",
      "\n",
      "Test's ROC AUC: 0.96233\n",
      "Test's logloss: 0.18167\n"
     ]
    }
   ],
   "source": [
    "fit = lgb.Dataset(X_fit, y_fit)\n",
    "val = lgb.Dataset(X_val, y_val, reference=fit)\n",
    "\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'binary'\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {metrics.roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {metrics.log_loss(y_test, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
